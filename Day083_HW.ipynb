{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Day083_HW.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qIeTNuEdc3hq","colab_type":"text"},"source":["## Work\n","1. 試比較有 BN 在 Batch_size = 2, 16, 32, 128, 256 下的差異\n","2. 請嘗試將 BN 放在 Activation 之前，並比較訓練結果\n","3. 請於 BN 放在 Input Layer 後，並比較結果"]},{"cell_type":"code","metadata":{"id":"z13542xbc3ht","colab_type":"code","colab":{}},"source":["import os\n","import keras\n","import itertools\n","# Disable GPU\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgcBmTGgc3hw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"8388bc99-3cd7-4966-9d31-49fb03a2dd0f","executionInfo":{"status":"ok","timestamp":1575286850847,"user_tz":-480,"elapsed":10157,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}}},"source":["train, test = keras.datasets.cifar10.load_data()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 6s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j16JRomVc3hz","colab_type":"code","colab":{}},"source":["## 資料前處理\n","def preproc_x(x, flatten=True):\n","    x = x / 255.\n","    if flatten:\n","        x = x.reshape((len(x), -1))\n","    return x\n","\n","def preproc_y(y, num_classes=10):\n","    if y.shape[-1] == 1:\n","        y = keras.utils.to_categorical(y, num_classes)\n","    return y    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eG-epGyic3h4","colab_type":"code","colab":{}},"source":["x_train, y_train = train\n","x_test, y_test = test\n","\n","# Preproc the inputs\n","x_train = preproc_x(x_train)\n","x_test = preproc_x(x_test)\n","\n","# Preprc the outputs\n","y_train = preproc_y(y_train)\n","y_test = preproc_y(y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fM8JNruc3h6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c94b66d9-faab-4e32-8ab9-483e75d5f147","executionInfo":{"status":"error","timestamp":1575292207088,"user_tz":-480,"elapsed":1739199,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}}},"source":["from keras.layers import BatchNormalization\n","\n","\"\"\"\n","建立神經網路，並加入 BN layer\n","\"\"\"\n","def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128]):\n","    \n","    input_layer = keras.layers.Input(input_shape)\n","    \n","    for i, n_units in enumerate(num_neurons):\n","        if i == 0:\n","            x = keras.layers.Dense(units=n_units, \n","                                   activation=\"relu\", \n","                                   name=\"hidden_layer\"+str(i+1))(input_layer)\n","            x = BatchNormalization()(x)\n","        else:\n","            x = keras.layers.Dense(units=n_units, \n","                                   activation=\"relu\", \n","                                   name=\"hidden_layer\"+str(i+1))(x)\n","            x = BatchNormalization()(x)\n","    \n","    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n","    \n","    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n","    return model\n","\n","## 超參數設定\n","LEARNING_RATE = 1e-3\n","EPOCHS = 50\n","BATCH_SIZE = [2, 16, 32, 128, 256]\n","MOMENTUM = 0.95\n","\n","# Define results\n","results = {}\n","for batchSize in BATCH_SIZE :\n","    \n","    model = build_mlp(input_shape=x_train.shape[1:])\n","    model.summary()\n","\n","    optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n","    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n","\n","    model.fit(x_train, y_train, \n","              epochs=EPOCHS, \n","              \n","              # different batch_size\n","              batch_size=batchSize, \n","              \n","              validation_data=(x_test, y_test), \n","              shuffle=True)\n","\n","    # Collect results\n","    train_loss = model.history.history[\"loss\"]\n","    valid_loss = model.history.history[\"val_loss\"]\n","    train_acc = model.history.history[\"acc\"]\n","    valid_acc = model.history.history[\"val_acc\"]\n","    \n","    name_tag = 'batchSize : %.2f' % batchSize\n","    results[name_tag] = {'train-loss': train_loss,\n","                         'valid-loss': valid_loss,\n","                         'train-acc' : train_acc,\n","                         'valid-acc' : valid_acc}\n","  \n","    \n","    \n","    # plot\n","    import matplotlib.pyplot as plt\n","    %matplotlib inline\n","\n","    plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n","    plt.plot(range(len(valid_loss)), valid_loss, label=\"valid loss\")\n","    plt.legend()\n","    plt.title(\"Loss\")\n","    plt.show()\n","\n","    plt.plot(range(len(train_acc)), train_acc, label=\"train accuracy\")\n","    plt.plot(range(len(valid_acc)), valid_acc, label=\"valid accuracy\")\n","    plt.legend()\n","    plt.title(\"Accuracy\")\n","    plt.show()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 3072)              0         \n","_________________________________________________________________\n","hidden_layer1 (Dense)        (None, 512)               1573376   \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","hidden_layer2 (Dense)        (None, 256)               131328    \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 256)               1024      \n","_________________________________________________________________\n","hidden_layer3 (Dense)        (None, 128)               32896     \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 128)               512       \n","_________________________________________________________________\n","output (Dense)               (None, 10)                1290      \n","=================================================================\n","Total params: 1,742,474\n","Trainable params: 1,740,682\n","Non-trainable params: 1,792\n","_________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Train on 50000 samples, validate on 10000 samples\n","Epoch 1/50\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","50000/50000 [==============================] - 272s 5ms/step - loss: 2.3047 - acc: 0.1493 - val_loss: 2.2911 - val_acc: 0.1771\n","Epoch 2/50\n","50000/50000 [==============================] - 268s 5ms/step - loss: 2.2315 - acc: 0.1694 - val_loss: 2.2552 - val_acc: 0.2255\n","Epoch 3/50\n","50000/50000 [==============================] - 267s 5ms/step - loss: 2.2350 - acc: 0.1683 - val_loss: 2.5511 - val_acc: 0.1498\n","Epoch 4/50\n","50000/50000 [==============================] - 265s 5ms/step - loss: 2.2390 - acc: 0.1602 - val_loss: 2.6641 - val_acc: 0.1635\n","Epoch 5/50\n","50000/50000 [==============================] - 266s 5ms/step - loss: 2.2301 - acc: 0.1694 - val_loss: 2.5392 - val_acc: 0.1840\n","Epoch 6/50\n","50000/50000 [==============================] - 266s 5ms/step - loss: 2.2264 - acc: 0.1665 - val_loss: 3.0315 - val_acc: 0.2160\n","Epoch 7/50\n","50000/50000 [==============================] - 265s 5ms/step - loss: 2.2254 - acc: 0.1650 - val_loss: 3.0075 - val_acc: 0.2161\n","Epoch 8/50\n","50000/50000 [==============================] - 265s 5ms/step - loss: 2.2233 - acc: 0.1685 - val_loss: 2.7398 - val_acc: 0.2268\n","Epoch 9/50\n","50000/50000 [==============================] - 265s 5ms/step - loss: 2.2331 - acc: 0.1676 - val_loss: 2.7735 - val_acc: 0.2068\n","Epoch 10/50\n","50000/50000 [==============================] - 266s 5ms/step - loss: 2.2440 - acc: 0.1619 - val_loss: 2.7765 - val_acc: 0.1889\n","Epoch 11/50\n","50000/50000 [==============================] - 265s 5ms/step - loss: 2.2522 - acc: 0.1575 - val_loss: 2.5883 - val_acc: 0.1878\n","Epoch 12/50\n","50000/50000 [==============================] - 271s 5ms/step - loss: 2.2463 - acc: 0.1634 - val_loss: 2.7079 - val_acc: 0.1918\n","Epoch 13/50\n","29538/50000 [================>.............] - ETA: 1:50 - loss: 2.2520 - acc: 0.1575Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rJBADkMsdRkP","colab_type":"text"},"source":["觀察現象 : BATCH_SIZE = [2, 16, 32, 128, 256]¶¶\n","when batch_size == 2, low train acc(0.1264), low validation acc(0.1390), there's no overfitting\n","\n","when batch_size == 16, a better train acc(0.6089), better validation acc(0.5367), overfitting exists\n","\n","when batch_size == 32, 128, 256, high train acc(0.9, 0.95), poor validation acc(0.45), overfitting"]},{"cell_type":"markdown","metadata":{"id":"_BRxtjeidX0A","colab_type":"text"},"source":["2.請嘗試將 BN 放在 Activation 之前，並比較訓練結果"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"v_WZtdGpc3h_","colab_type":"code","colab":{}},"source":["import os\n","import keras\n","import itertools\n","# Disable GPU\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n","\n","train, test = keras.datasets.cifar10.load_data()\n","\n","## 資料前處理\n","def preproc_x(x, flatten=True):\n","    x = x / 255.\n","    if flatten:\n","        x = x.reshape((len(x), -1))\n","    return x\n","\n","def preproc_y(y, num_classes=10):\n","    if y.shape[-1] == 1:\n","        y = keras.utils.to_categorical(y, num_classes)\n","    return y    \n","\n","x_train, y_train = train\n","x_test, y_test = test\n","\n","# Preproc the inputs\n","x_train = preproc_x(x_train)\n","x_test = preproc_x(x_test)\n","\n","# Preprc the outputs\n","y_train = preproc_y(y_train)\n","y_test = preproc_y(y_test)\n","\n","\n","from keras.layers import BatchNormalization\n","\n","\"\"\"\n","建立神經網路，並加入 BN layer\n","\"\"\"\n","def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128]):\n","    \n","    input_layer = keras.layers.Input(input_shape)\n","    \n","    for i, n_units in enumerate(num_neurons):\n","        if i == 0:\n","            # 2. 把 BatchNormalization 放在 activation 前\n","            x = BatchNormalization()(input_layer)\n","            \n","            x = keras.layers.Dense(units=n_units, \n","                                   activation=\"relu\", \n","                                   name=\"hidden_layer\"+str(i+1))(x)\n","        else:\n","            # 2. 把 BatchNormalization 放在 activation 前\n","            x = BatchNormalization()(x)\n","            \n","            x = keras.layers.Dense(units=n_units, \n","                                   activation=\"relu\", \n","                                   name=\"hidden_layer\"+str(i+1))(x)\n","    \n","    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n","    \n","    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n","    return model\n","\n","\n","## 超參數設定\n","LEARNING_RATE = 1e-3\n","EPOCHS = 50\n","BATCH_SIZE = 512\n","MOMENTUM = 0.95\n","\n","model = build_mlp(input_shape=x_train.shape[1:])\n","model.summary()\n","\n","optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n","model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n","\n","model.fit(x_train, y_train, \n","          epochs=EPOCHS, \n","          batch_size=BATCH_SIZE, \n","          validation_data=(x_test, y_test), \n","          shuffle=True)\n","\n","# Collect results\n","train_loss = model.history.history[\"loss\"]\n","valid_loss = model.history.history[\"val_loss\"]\n","train_acc = model.history.history[\"acc\"]\n","valid_acc = model.history.history[\"val_acc\"]\n","   \n","    \n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n","plt.plot(range(len(valid_loss)), valid_loss, label=\"valid loss\")\n","plt.legend()\n","plt.title(\"Loss\")\n","plt.show()\n","\n","plt.plot(range(len(train_acc)), train_acc, label=\"train accuracy\")\n","plt.plot(range(len(valid_acc)), valid_acc, label=\"valid accuracy\")\n","plt.legend()\n","plt.title(\"Accuracy\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZAx8YxLMdf_k","colab_type":"text"},"source":["3.請於 BN 放在 Input Layer 後，並比較結果"]},{"cell_type":"code","metadata":{"id":"n8LWc9Yjc3iB","colab_type":"code","colab":{}},"source":["import os\n","import keras\n","import itertools\n","# Disable GPU\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n","train, test = keras.datasets.cifar10.load_data()\n","\n","## 資料前處理\n","def preproc_x(x, flatten=True):\n","    x = x / 255.\n","    if flatten:\n","        x = x.reshape((len(x), -1))\n","    return x\n","\n","def preproc_y(y, num_classes=10):\n","    if y.shape[-1] == 1:\n","        y = keras.utils.to_categorical(y, num_classes)\n","    return y    \n","\n","x_train, y_train = train\n","x_test, y_test = test\n","\n","# Preproc the inputs\n","x_train = preproc_x(x_train)\n","x_test = preproc_x(x_test)\n","\n","# Preprc the outputs\n","y_train = preproc_y(y_train)\n","y_test = preproc_y(y_test)\n","from keras.layers import BatchNormalization\n","\n","\"\"\"\n","建立神經網路，並加入 BN layer\n","\"\"\"\n","def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128]):\n","    \n","    input_layer = keras.layers.Input(input_shape)\n","    \n","    # 3. BN 放在 Input Layer 後\n","    x = BatchNormalization()(input_layer)\n","    \n","    for i, n_units in enumerate(num_neurons):\n","        if i == 0:\n","            x = keras.layers.Dense(units=n_units, \n","                                   activation=\"relu\", \n","                                   name=\"hidden_layer\"+str(i+1))(x)\n","        else:\n","            x = keras.layers.Dense(units=n_units, \n","                                   activation=\"relu\", \n","                                   name=\"hidden_layer\"+str(i+1))(x)\n","    \n","    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n","    \n","    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n","    return model\n","\n","## 超參數設定\n","LEARNING_RATE = 1e-3\n","EPOCHS = 50\n","BATCH_SIZE = 512\n","MOMENTUM = 0.95\n","\n","model = build_mlp(input_shape=x_train.shape[1:])\n","model.summary()\n","\n","optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=MOMENTUM)\n","model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n","\n","model.fit(x_train, y_train, \n","          epochs=EPOCHS, \n","          batch_size=BATCH_SIZE, \n","          validation_data=(x_test, y_test), \n","          shuffle=True)\n","\n","# Collect results\n","train_loss = model.history.history[\"loss\"]\n","valid_loss = model.history.history[\"val_loss\"]\n","train_acc = model.history.history[\"acc\"]\n","valid_acc = model.history.history[\"val_acc\"]\n","   \n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.plot(range(len(train_loss)), train_loss, label=\"train loss\")\n","plt.plot(range(len(valid_loss)), valid_loss, label=\"valid loss\")\n","plt.legend()\n","plt.title(\"Loss\")\n","plt.show()\n","\n","plt.plot(range(len(train_acc)), train_acc, label=\"train accuracy\")\n","plt.plot(range(len(valid_acc)), valid_acc, label=\"valid accuracy\")\n","plt.legend()\n","plt.title(\"Accuracy\")\n","plt.show()"],"execution_count":0,"outputs":[]}]}